# Project_yolov7_tomato_detection
 YOLOv7 is the fastest and most accurate real-time object detection model for computer vision tasks. The official paper demonstrates how this improved architecture surpasses all previous YOLO versions — as well as all other object detection models — in terms of both speed and accuracy on the MS COCO dataset; achieving this performance without utilizing any pretrained weights. Additionally, following all of the controversy around the naming conventions of previous YOLO models, as YOLOv7 was released by the same authors that developed Scaled-YOLOv4, the machine learning community seems happy to accept this as the next iteration of the ‘official’ YOLO family!

At the point that YOLOv7 was released, we — as part of Microsoft’s Data & AI Service Line — were partway through a challenging object-detection based customer project, in a domain drastically different to COCO. Needless to say, both ourselves and the customer were very excited at the prospect of applying YOLOv7 to our problem. Unfortunately, when using the out-of-the-box settings, the results were … let’s just say, not great.

After reading the official paper, we found that, whilst it presents a comprehensive overview on the architectural changes, it omits many details around how the model was trained; for example, which data augmentation techniques were applied, and how the loss function measures that the model is doing a good job! To understand these technicalities, we decided to debug the code directly. However, as the YOLOv7 repository is a modified fork of the YOLOR codebase — which itself is a fork of YOLOv5 — we found that it includes a lot of complex functionality, much of which is not needed when just training a model; for example, being able to specify custom architectures in Yaml format and have these translated into PyTorch models. Additionally, the codebase contains many custom components which have been implemented from scratch — such as a multi-GPU training loop, several data augmentations, samplers to preserve dataloader workers, and multiple learning rate schedulers — many of which are now available in PyTorch, or other libraries. As a result, there was a lot of code to dissect; it took us a long time to understand how everything worked, as well as the intricacies of the training loop which contribute to the model’s excellent performance! Eventually, with this understanding, we were able to set up our training recipe to obtain consistently good results on our task.

In this article, we intend to take a practical approach in demonstrating how to train YOLOv7 models in custom training scripts, as well as exploring areas such as data augmentation techniques, how to select and modify anchor boxes, and demystifying how the loss function works; (hopefully!) enabling you to build up an intuition of what is likely to work well for your own problems. As the YOLOv7 architecture is well described in detail in the official paper, as well as in many other sources, we are not going to cover this here. Instead, we intend to focus on all of the other details which, whilst contribute to YOLOv7’s performance, are not covered in the paper. This tends to be knowledge which has been accumulated over multiple versions of YOLO models but can be incredibly difficult to track down for someone just entering the field.

To illustrate these concepts, we shall be using our own implementation of YOLOv7, which utilises the official pretrained weights, but has been written with modularity and readability in mind. This project initially started as an exercise for us to improve our understanding of how YOLOv7 works under the hood — in order to better understand how to apply it — but after successfully using it on a few different tasks, we have decided to make it publicly available. Whilst we would recommend using the official implementation if you wish to exactly reproduce the published results on COCO, we find that this implementation is more flexible to apply, and extend, to custom domains. Hopefully, this implementation will provide a clean and clear starting point for anyone wishing to experiment with YOLOv7 in their own custom training scripts, as well as providing more transparency around the techniques that were used during training in the original implementation.
